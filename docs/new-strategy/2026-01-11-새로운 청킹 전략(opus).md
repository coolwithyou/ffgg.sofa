# RAG 문서 전처리 전략: LLM 재구성 + Human-in-the-loop 검증

## 1. 배경 및 문제 정의

### 1.1 현재 청킹 방식의 한계

기존의 의미 단위/문단 기반 청킹은 다음과 같은 구조적 문제를 갖는다:

| 문제 유형 | 예시 | 원인 |
|-----------|------|------|
| **맥락 단절** | 마케팅부서 연락처 30개 → 10개씩 분리 | 청킹이 구조가 아닌 길이/위치 기반 |
| **헤더 분리** | "영업팀 정책" 헤더와 본문이 다른 청크 | 줄바꿈/공백을 의미 경계로 오인 |
| **암묵적 참조 손실** | "위 조건에 해당하면..." → '위 조건'이 다른 청크 | 문서 내 참조 관계 무시 |
| **테이블 파편화** | 표의 헤더와 데이터 행이 분리됨 | 시각적 구조 인식 불가 |

### 1.2 핵심 요구사항

- **정확성 우선**: 시장 진입 단계에서 "제대로 작동하는 시스템"이 비용 최적화보다 중요
- **경쟁 차별화**: 대부분의 RAG SaaS가 "알아서 됨"을 팔지만 실제로는 안 됨
- **B2B 신뢰**: 기업 고객은 "AI가 알아서 했어요"보다 "AI가 정리하고 당신이 확인했습니다"를 선호

---

## 2. 제안 솔루션: LLM 재구성 + 검증 워크플로우

### 2.1 핵심 개념

사용자가 업로드한 문서를 LLM이 마크다운 형태로 재구성한 후, **청킹 전에 사용자 검증 단계**를 거치는 방식.

```
[원본 문서] → [LLM 재구성] → [비교/검증 UI] → [사용자 승인] → [청킹] → [벡터 DB]
```

### 2.2 왜 이 접근이 유효한가

1. **Human-in-the-loop이 환각 문제의 가장 확실한 해답**
   - LLM 환각을 LLM으로 검증하는 건 근본적 한계가 있음
   - 사람이 직접 보는 단계를 넣으면 책임 소재도 명확해짐

2. **화이트 글러브 모델과 자연스럽게 연결**
   - 내부 QA 프로세스로 사용 (고객에게 안 보여줌)
   - 또는 고객 승인 단계로 사용 (투명성 강조)
   - 초기엔 내부 QA → 안정화 후 고객 셀프서비스로 전환 가능

3. **변환 과정의 투명성 자체가 신뢰 포인트**
   - 사용자는 "내 문서가 어떻게 학습되는지"를 통제한다는 느낌을 받음
   - B2B에서 신뢰/책임소재/감사(audit) 측면이 강화됨

---

## 3. 기술적 구현 전략

### 3.1 문서 재구성 접근법 (3단계)

#### Stage 1: Contextual Prefix (기본)
각 청크 생성 시, 문서 전체를 보고 해당 청크의 맥락을 요약해서 앞에 붙임.

```python
for chunk in raw_chunks:
    context = llm.generate(
        f"다음은 '{doc_title}'의 일부입니다. "
        f"이 청크가 문서 내에서 어떤 맥락인지 2문장으로 설명하세요:\n{chunk}"
    )
    enriched_chunk = f"{context}\n---\n{chunk}"
```

- **장점**: 원본 보존, 검색 품질 향상, 구현 단순
- **비용**: 청크당 100-200 토큰 추가

#### Stage 2: 구조 인식 전처리
문서 전체를 먼저 분석해서 구조 맵을 만들고, 그 구조에 따라 청킹.

```
1. 문서 → LLM → 구조 JSON 추출
   {"sections": [{"title": "마케팅부서 연락처", "range": [15, 45]}...]}
   
2. 구조 기반으로 청킹 (섹션 단위 우선)

3. 각 청크에 구조 메타데이터 태깅
```

#### Stage 3: 선택적 재구성
테이블, 연락처 목록 등 **정형 데이터**만 LLM으로 정규화.

```
원본: 
"김철수
 02-1234-5678
 
 박영희  031-5678-1234"

재구성:
"[마케팅부서 연락처]
- 김철수: 02-1234-5678
- 박영희: 031-5678-1234"
```

### 3.2 AI 사실관계 검증 파이프라인

단순 "두 문서 비교해줘"는 환각 검증에 위험. 다음과 같이 파이프라인을 분리:

#### (A) Claim 추출
재구성 문서에서 검증 가능한 주장만 추출:
- "마케팅팀 연락처는 30개다"
- "홍길동의 이메일은 x@company.com"
- "환불 정책은 7일 이내다"

#### (B) 근거 스팬 매핑
각 claim에 대해 원문에서 근거 스팬을 검색/지정:
- LLM이 "근거 후보 구간"을 1~3개 제시
- 해당 스팬을 UI에서 바로 열람 가능하게

#### (C) 3값 판정
- `SUPPORTED`: 근거 있음
- `CONTRADICTED`: 근거와 충돌
- `NOT FOUND`: 근거 못 찾음 / 애매함

#### (D) 검증 레벨 분리
```python
Level 1: 정규식 기반 - 전화번호, 이메일, 금액 등 추출 후 1:1 매칭
Level 2: LLM 기반 - "원본에 있는데 재구성본에 없는 정보가 있나요?"
Level 3: 사람 검토 - Level 1,2에서 불일치 발생한 부분만
```

---

## 4. UI/UX 설계

### 4.1 듀얼 뷰어 구조

```
┌─────────────────────────────────────────────────────────────┐
│  [필터] Added | Missing | Moved | Contradicted | All        │
├────────────────────────┬────────────────────────────────────┤
│                        │                                    │
│   원본 문서 뷰어        │   재구성 마크다운 에디터            │
│   (PDF/이미지 렌더)     │   (수정 가능)                      │
│                        │                                    │
│   [하이라이트 영역]     │   [근거 점프 버튼]                  │
│                        │                                    │
├────────────────────────┴────────────────────────────────────┤
│  검증 리스트                                                 │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ ⚠️ HIGH | "연락처 수 30개" | CONTRADICTED | [점프]    │   │
│  │ ✓ LOW  | "부서명: 마케팅팀" | SUPPORTED | [점프]     │   │
│  └─────────────────────────────────────────────────────┘   │
├─────────────────────────────────────────────────────────────┤
│                    [검증 완료 및 저장]                        │
└─────────────────────────────────────────────────────────────┘
```

### 4.2 필수 UI 요소 (6개)

1. **섹션별 "근거 점프" 버튼**: 클릭하면 원문 해당 위치로 이동
2. **Added / Missing / Moved / Contradicted 필터 탭**: 리스크 유형별 필터링
3. **Claim 단위 검증 리스트**: 한 줄 요약 + 판정 + 근거 링크
4. **동기 하이라이트**: 우측 항목 hover → 좌측 원문 영역 강조
5. **변경 승인 체크**: 특히 High risk 항목은 필수 체크박스
6. **원문 기반 답변 테스트 버튼**: 샘플 Q 5개 자동 생성해서 RAG 결과 품질 확인

### 4.3 검수 피로 방지 전략

**문제**: 문서가 길면 결국 "대충 넘어감"

**해결책**:
- 전체를 다 보게 하지 않음
- AI가 1차 필터링 → 사람은 플래그된 부분만 집중 검토
- 위험 점수(High/Med/Low) 기반으로 High만 필수 체크

```
문서 레벨: 신뢰도 점수 낮으면 → 검토 큐에 진입
항목 레벨: High risk 항목만 → 필수 체크박스
나머지: "자동 승인" 가능
```

---

## 5. 리스크 및 대응책

### 5.1 식별된 리스크

| 리스크 | 심각도 | 대응책 |
|--------|--------|--------|
| **검수 피로로 인한 형식적 승인** | High | 자동 검증 리포트 + 고위험 항목만 필수 검수 |
| **정보 왜곡/손실** | High | 원본 보존 + 메타데이터 증강 방식 병행 |
| **원본 텍스트 추출 오류** | Medium | PDF 렌더 + 추출 텍스트 오버레이 동시 표시 |
| **민감정보 노출 증가** | Medium | 마스킹 옵션 + 권한 제어 + 감사 로그 |
| **비교 기능 복잡도** | Medium | Progressive disclosure - 기본은 간단히, 위험 시에만 고급 비교 |
| **처리 비용 증가** | Low | 문서 타입별 선택적 적용 |

### 5.2 실패 시나리오별 대응

#### 시나리오 1: 사용자가 검수를 안 한다
- 자동 검증 리포트 생성 필수화
- High risk 항목 체크 없이는 저장 불가

#### 시나리오 2: 원본 추출이 이미 깨져있다
- 원문 뷰를 "PDF 렌더" + "추출 텍스트 오버레이" 같이 표시
- 사용자가 "원본"이라 믿지만 실제론 추출본임을 인지시킴

#### 시나리오 3: 환각이 그럴듯해서 못 잡는다
- Claim 단위로 쪼개서 검증
- 숫자/연락처/날짜는 정규식으로 1:1 매칭 (LLM 의존 안 함)

---

## 6. 구현 로드맵

### Phase 1: MVP (2주)
- 좌: 원본 PDF 뷰어
- 우: 마크다운 에디터 (수정 가능)
- AI 생성 "의심 항목 리스트" (숫자, 연락처, 날짜 불일치만)
- 항목 클릭 → 원문 페이지로 점프
- [승인] 버튼 → 청킹 진행

**스코프 제한**: 스크롤 동기화 없음, 수동 탐색

### Phase 2: 검증 고도화 (검증 후 2주)
- Added/Missing/Contradicted 필터
- Claim 단위 검증 리포트
- 감사 로그
- 문서 타입별 재구성 템플릿

### Phase 3: 고객 피드백 기반 (이후)
- 스크롤 동기화
- 민감정보 자동 마스킹
- 버전 관리 (원본 → 재구성 → 수정 → 최종)
- 고객 셀프서비스 검토 기능

---

## 7. 버전 관리 구조

```
문서 ID: doc_abc123
├─ v1: 원본 (업로드 시점)
├─ v2: AI 재구성 (자동 생성)
├─ v3: 사용자 수정 (검토 후)
└─ v3_chunked: 최종 청킹 결과
```

---

## 8. 문서 타입별 전략 (추후 구체화 필요)

| 문서 타입 | 재구성 방식 | 검증 초점 | 위험 수준 |
|-----------|-------------|-----------|-----------|
| **연락처 목록** | 정형화된 리스트로 변환 | 숫자/이메일 1:1 매칭 | High (팩트 오류 치명적) |
| **정책/규정 문서** | 원본 보존 + 메타데이터 | 조항 누락 여부 | High (법적 리스크) |
| **FAQ** | Q&A 쌍으로 구조화 | 답변-질문 매칭 | Medium |
| **매뉴얼/가이드** | 섹션 구조 유지 | 절차 순서 보존 | Medium |
| **회의록** | 시간순 정리 | 참석자/결정사항 | Low |

---

## 9. 다음 단계

### 즉시 결정 필요
1. **MVP 대상 문서 타입**: 연락처 목록으로 시작 권장 (검증 가장 명확)
2. **내부 QA vs 고객 노출**: 초기에는 내부 QA로 운영 후 안정화

### 후속 작업
1. Phase 1 "의심 항목 추출" 프롬프트 설계
2. 재구성 출력 스키마 정의 (문서 타입별)
3. 위험 점수 기준 정의 (숫자/연락처/정책 문장 = High)
4. UI 와이어프레임 상세 설계

---

## 10. 참고 자료

- [Anthropic Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval) - 검색 실패율 49% 감소 보고
- Unstructured.io, LlamaParse - 구조화된 문서 추출 도구 (바퀴 재발명 방지)

---

*문서 작성일: 2026-01-11*
*상태: 논의 정리 완료, Phase 1 설계 대기*