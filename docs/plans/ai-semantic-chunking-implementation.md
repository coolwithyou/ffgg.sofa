# AI Semantic Chunking êµ¬í˜„ ê³„íš

> **ì‘ì„±ì¼**: 2025-01-09
> **ìƒíƒœ**: ì„¤ê³„ ì™„ë£Œ, êµ¬í˜„ ëŒ€ê¸°
> **ì˜ˆìƒ ê¸°ê°„**: 1ì£¼

---

## 1. í˜„ì¬ ìƒíƒœ ë¶„ì„

### 1.1 ì´ë¯¸ êµ¬í˜„ëœ ê¸°ëŠ¥

| ê¸°ëŠ¥ | íŒŒì¼ | ìƒíƒœ |
|------|------|------|
| ê·œì¹™ ê¸°ë°˜ ì²­í‚¹ | `lib/rag/chunking.ts` | âœ… ìš´ì˜ ì¤‘ |
| Contextual Retrieval | `lib/rag/context.ts` | âœ… ìš´ì˜ ì¤‘ |
| Hybrid Search (Vector + BM25) | `lib/rag/retrieval.ts` | âœ… ìš´ì˜ ì¤‘ |
| LLM Reranking | `lib/rag/reranker.ts` | âœ… ìš´ì˜ ì¤‘ |
| ì„ë² ë”© ìƒì„± | `lib/rag/embedding.ts` | âœ… ìš´ì˜ ì¤‘ |

### 1.2 ì¶”ê°€ í•„ìš”í•œ ê¸°ëŠ¥

| ê¸°ëŠ¥ | íŒŒì¼ | ìƒíƒœ |
|------|------|------|
| **AI Semantic Chunking** | `lib/rag/semantic-chunking.ts` | ğŸ†• ì‹ ê·œ ê°œë°œ |

---

## 2. ì•„í‚¤í…ì²˜ ì„¤ê³„

### 2.1 íŒŒì´í”„ë¼ì¸ ë¹„êµ

```
[í˜„ì¬]
ë¬¸ì„œ â†’ smartChunk (ê·œì¹™) â†’ Contextual â†’ ì„ë² ë”© â†’ ì €ì¥

[ê°œì„ ]
ë¬¸ì„œ â†’ preChunk (ê·œì¹™, í° ë‹¨ìœ„) â†’ semanticChunk (AI) â†’ Contextual â†’ ì„ë² ë”© â†’ ì €ì¥
```

### 2.2 ëª¨ë“ˆ êµ¬ì¡°

```
lib/rag/
â”œâ”€â”€ chunking.ts              # ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ (ìœ ì§€)
â”œâ”€â”€ semantic-chunking.ts     # ğŸ†• AI ê¸°ë°˜ ì²­í‚¹
â”œâ”€â”€ context.ts               # Contextual Retrieval (ìœ ì§€)
â”œâ”€â”€ embedding.ts             # ì„ë² ë”© (ìœ ì§€)
â”œâ”€â”€ retrieval.ts             # Hybrid Search (ìœ ì§€)
â””â”€â”€ reranker.ts              # LLM Reranking (ìœ ì§€)
```

---

## 3. ìƒì„¸ ì„¤ê³„

### 3.1 ì‹ ê·œ íŒŒì¼: `lib/rag/semantic-chunking.ts`

```typescript
// lib/rag/semantic-chunking.ts

import { createAnthropic } from '@ai-sdk/anthropic';
import { generateText } from 'ai';
import { logger } from '@/lib/logger';
import { trackTokenUsage } from '@/lib/usage/token-tracker';

// ============================================================
// íƒ€ì… ì •ì˜
// ============================================================

export interface SemanticChunk {
  content: string;
  type: 'paragraph' | 'qa' | 'list' | 'table' | 'header' | 'code';
  topic: string;
  index: number;
  metadata: {
    startOffset: number;
    endOffset: number;
    originalSegmentIndex: number;
  };
}

export interface SemanticChunkOptions {
  minChunkSize?: number;      // ìµœì†Œ ì²­í¬ í¬ê¸° (ê¸°ë³¸: 100ì)
  maxChunkSize?: number;      // ìµœëŒ€ ì²­í¬ í¬ê¸° (ê¸°ë³¸: 600ì)
  preChunkSize?: number;      // 1ì°¨ ë¶„í•  í¬ê¸° (ê¸°ë³¸: 2000ì)
  model?: string;             // AI ëª¨ë¸ (ê¸°ë³¸: claude-3-haiku)
  batchSize?: number;         // ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 5)
  batchDelayMs?: number;      // ë°°ì¹˜ ê°„ ë”œë ˆì´ (ê¸°ë³¸: 100ms)
}

interface SemanticChunkResult {
  content: string;
  type: string;
  topic: string;
}

// ============================================================
// ìƒìˆ˜
// ============================================================

const SEMANTIC_MODEL = 'claude-3-haiku-20240307';

const DEFAULT_OPTIONS: Required<SemanticChunkOptions> = {
  minChunkSize: 100,
  maxChunkSize: 600,
  preChunkSize: 2000,
  model: SEMANTIC_MODEL,
  batchSize: 5,
  batchDelayMs: 100,
};

const SEMANTIC_CHUNK_PROMPT_KO = `<segment>
{{SEGMENT}}
</segment>

ìœ„ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ ì™„ê²°ëœ ì²­í¬ë“¤ë¡œ ë¶„í• í•˜ì„¸ìš”.

## ë¶„í•  ê·œì¹™
1. ê° ì²­í¬ëŠ” í•˜ë‚˜ì˜ ì™„ê²°ëœ ê°œë…/ì£¼ì œë¥¼ ë‹´ì•„ì•¼ í•¨
2. Q&A ìŒ(ì§ˆë¬¸+ë‹µë³€)ì€ ë°˜ë“œì‹œ í•¨ê»˜ ìœ ì§€
3. ëª©ë¡ì€ ê°€ëŠ¥í•œ í•œ ë‹¨ìœ„ë¡œ ìœ ì§€ (ë„ˆë¬´ ê¸¸ë©´ ë…¼ë¦¬ì  ë‹¨ìœ„ë¡œ ë¶„í• )
4. í‘œëŠ” ë¶„í• í•˜ì§€ ì•ŠìŒ
5. ì½”ë“œ ë¸”ë¡ì€ ë¶„í• í•˜ì§€ ì•ŠìŒ
6. 100-600ì ê¶Œì¥ (ì˜ë¯¸ ì™„ê²°ì„±ì´ ë¬¸ì ìˆ˜ë³´ë‹¤ ìš°ì„ )
7. ë¬¸ì¥ ì¤‘ê°„ì—ì„œ ì ˆëŒ€ ìë¥´ì§€ ë§ ê²ƒ

## ì²­í¬ íƒ€ì…
- paragraph: ì¼ë°˜ ë¬¸ë‹¨
- qa: Q&A ìŒ
- list: ëª©ë¡
- table: í‘œ
- header: ì œëª© + ì„¤ëª…
- code: ì½”ë“œ ë¸”ë¡

## ì¶œë ¥ í˜•ì‹
JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
[
  {"content": "ì²­í¬ ë‚´ìš©", "type": "paragraph", "topic": "ì£¼ì œ í‚¤ì›Œë“œ"},
  {"content": "Q: ì§ˆë¬¸\\nA: ë‹µë³€", "type": "qa", "topic": "FAQ ì£¼ì œ"}
]`;

const SEMANTIC_CHUNK_PROMPT_EN = `<segment>
{{SEGMENT}}
</segment>

Split the text above into semantically complete chunks.

## Splitting Rules
1. Each chunk should contain one complete concept/topic
2. Q&A pairs (question + answer) must stay together
3. Keep lists as single units when possible (split logically if too long)
4. Do not split tables
5. Do not split code blocks
6. Target 100-600 characters (semantic completeness > character count)
7. Never split in the middle of a sentence

## Chunk Types
- paragraph: general paragraph
- qa: Q&A pair
- list: list/enumeration
- table: table
- header: heading + description
- code: code block

## Output Format
Output only a JSON array. No other explanation.
[
  {"content": "chunk content", "type": "paragraph", "topic": "topic keyword"},
  {"content": "Q: question\\nA: answer", "type": "qa", "topic": "FAQ topic"}
]`;

// ============================================================
// í—¬í¼ í•¨ìˆ˜
// ============================================================

/**
 * í•œêµ­ì–´ ë¬¸ì„œì¸ì§€ íŒë³„
 */
function isKoreanDocument(text: string): boolean {
  const koreanChars = text.match(/[ê°€-í£]/g) || [];
  return koreanChars.length > text.length * 0.1;
}

/**
 * 1ì°¨ ê·œì¹™ ê¸°ë°˜ ë¶„í•  (í° ë‹¨ìœ„)
 */
function preChunk(content: string, maxSize: number): string[] {
  const segments: string[] = [];

  // 1. ë¨¼ì € í° êµ¬ë¶„ìë¡œ ë¶„í•  ì‹œë„ (í—¤ë”, ë¹ˆ ì¤„ 2ê°œ ì´ìƒ)
  const majorSplits = content.split(/\n{3,}|(?=^#{1,3}\s)/gm);

  for (const split of majorSplits) {
    const trimmed = split.trim();
    if (!trimmed) continue;

    if (trimmed.length <= maxSize) {
      segments.push(trimmed);
    } else {
      // í° ì„¸ê·¸ë¨¼íŠ¸ëŠ” ë‹¨ë½ ë‹¨ìœ„ë¡œ ì¬ë¶„í• 
      const paragraphs = trimmed.split(/\n{2,}/);
      let currentSegment = '';

      for (const para of paragraphs) {
        if ((currentSegment + '\n\n' + para).length <= maxSize) {
          currentSegment = currentSegment
            ? currentSegment + '\n\n' + para
            : para;
        } else {
          if (currentSegment) segments.push(currentSegment);
          currentSegment = para;
        }
      }
      if (currentSegment) segments.push(currentSegment);
    }
  }

  return segments.filter(s => s.length > 0);
}

/**
 * AI ì‘ë‹µ íŒŒì‹±
 */
function parseAIResponse(response: string): SemanticChunkResult[] {
  try {
    // JSON ë°°ì—´ ì¶”ì¶œ
    const jsonMatch = response.match(/\[[\s\S]*\]/);
    if (!jsonMatch) {
      throw new Error('No JSON array found in response');
    }

    const parsed = JSON.parse(jsonMatch[0]);

    if (!Array.isArray(parsed)) {
      throw new Error('Response is not an array');
    }

    return parsed.map((item: unknown) => {
      const obj = item as Record<string, unknown>;
      return {
        content: String(obj.content || ''),
        type: String(obj.type || 'paragraph'),
        topic: String(obj.topic || ''),
      };
    }).filter(chunk => chunk.content.length > 0);

  } catch (error) {
    logger.warn('Failed to parse AI response for semantic chunking', {
      error: error instanceof Error ? error.message : 'Unknown',
      responsePreview: response.slice(0, 200),
    });
    return [];
  }
}

// ============================================================
// ë©”ì¸ í•¨ìˆ˜
// ============================================================

/**
 * ë‹¨ì¼ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ AIë¡œ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• 
 */
async function chunkSegmentWithAI(
  segment: string,
  options: Required<SemanticChunkOptions>,
  trackingContext?: { tenantId: string }
): Promise<SemanticChunkResult[]> {
  const isKorean = isKoreanDocument(segment);
  const promptTemplate = isKorean ? SEMANTIC_CHUNK_PROMPT_KO : SEMANTIC_CHUNK_PROMPT_EN;
  const prompt = promptTemplate.replace('{{SEGMENT}}', segment);

  try {
    const anthropic = createAnthropic({
      apiKey: process.env.ANTHROPIC_API_KEY,
    });

    const result = await generateText({
      model: anthropic(options.model),
      prompt,
      maxOutputTokens: 4096,
      temperature: 0,
    });

    // í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
    if (trackingContext?.tenantId) {
      await trackTokenUsage({
        tenantId: trackingContext.tenantId,
        featureType: 'semantic_chunking',
        modelProvider: 'anthropic',
        modelId: options.model,
        inputTokens: result.usage?.inputTokens ?? 0,
        outputTokens: result.usage?.outputTokens ?? 0,
      });
    }

    const chunks = parseAIResponse(result.text);

    // ë¹ˆ ê²°ê³¼ë©´ ì›ë³¸ ë°˜í™˜
    if (chunks.length === 0) {
      return [{ content: segment, type: 'paragraph', topic: '' }];
    }

    return chunks;

  } catch (error) {
    logger.error('AI semantic chunking failed', error as Error, {
      segmentLength: segment.length,
    });

    // ì—ëŸ¬ ì‹œ ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
    return [{ content: segment, type: 'paragraph', topic: '' }];
  }
}

/**
 * AI ê¸°ë°˜ ì‹œë§¨í‹± ì²­í‚¹ í™œì„±í™” ì—¬ë¶€
 */
export function isSemanticChunkingEnabled(): boolean {
  return !!process.env.ANTHROPIC_API_KEY;
}

/**
 * ë©”ì¸ ì‹œë§¨í‹± ì²­í‚¹ í•¨ìˆ˜
 */
export async function semanticChunk(
  content: string,
  options: SemanticChunkOptions = {},
  onProgress?: (current: number, total: number) => void,
  trackingContext?: { tenantId: string }
): Promise<SemanticChunk[]> {
  const opts = { ...DEFAULT_OPTIONS, ...options };

  // API í‚¤ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
  if (!isSemanticChunkingEnabled()) {
    logger.info('Semantic chunking disabled (no API key), using rule-based');
    const { smartChunk } = await import('./chunking');
    const ruleBasedChunks = await smartChunk(content, {
      maxChunkSize: opts.maxChunkSize,
      overlap: 50,
      preserveStructure: true,
    });

    return ruleBasedChunks.map((chunk, index) => ({
      content: chunk.content,
      type: 'paragraph' as const,
      topic: '',
      index,
      metadata: {
        startOffset: chunk.metadata.startOffset || 0,
        endOffset: chunk.metadata.endOffset || chunk.content.length,
        originalSegmentIndex: 0,
      },
    }));
  }

  // 1. ê·œì¹™ ê¸°ë°˜ 1ì°¨ ë¶„í•  (í° ë‹¨ìœ„)
  const segments = preChunk(content, opts.preChunkSize);
  logger.info('Pre-chunking completed', { segmentCount: segments.length });

  // 2. AI ê¸°ë°˜ 2ì°¨ ë¶„í•  (ë°°ì¹˜ ì²˜ë¦¬)
  const allChunks: SemanticChunk[] = [];
  let globalIndex = 0;
  let globalOffset = 0;

  for (let i = 0; i < segments.length; i += opts.batchSize) {
    const batch = segments.slice(i, i + opts.batchSize);

    // ë°°ì¹˜ ë‚´ ë³‘ë ¬ ì²˜ë¦¬
    const batchResults = await Promise.all(
      batch.map((segment, batchIndex) =>
        chunkSegmentWithAI(segment, opts, trackingContext)
          .then(chunks => ({ segmentIndex: i + batchIndex, segment, chunks }))
      )
    );

    // ê²°ê³¼ ë³‘í•©
    for (const { segmentIndex, segment, chunks } of batchResults) {
      let segmentOffset = 0;

      for (const chunk of chunks) {
        allChunks.push({
          content: chunk.content,
          type: chunk.type as SemanticChunk['type'],
          topic: chunk.topic,
          index: globalIndex++,
          metadata: {
            startOffset: globalOffset + segmentOffset,
            endOffset: globalOffset + segmentOffset + chunk.content.length,
            originalSegmentIndex: segmentIndex,
          },
        });
        segmentOffset += chunk.content.length;
      }

      globalOffset += segment.length;
    }

    // ì§„í–‰ ìƒí™© ì½œë°±
    onProgress?.(Math.min(i + opts.batchSize, segments.length), segments.length);

    // ë°°ì¹˜ ê°„ ë”œë ˆì´ (rate limit ë°©ì§€)
    if (i + opts.batchSize < segments.length) {
      await new Promise(resolve => setTimeout(resolve, opts.batchDelayMs));
    }
  }

  // 3. í›„ì²˜ë¦¬: ë„ˆë¬´ ì§§ì€ ì²­í¬ ë³‘í•©
  const mergedChunks = mergeShortChunks(allChunks, opts.minChunkSize);

  // 4. ì¸ë±ìŠ¤ ì¬ì •ë ¬
  return mergedChunks.map((chunk, idx) => ({ ...chunk, index: idx }));
}

/**
 * ë„ˆë¬´ ì§§ì€ ì²­í¬ë¥¼ ì´ì „ ì²­í¬ì™€ ë³‘í•©
 */
function mergeShortChunks(
  chunks: SemanticChunk[],
  minSize: number
): SemanticChunk[] {
  if (chunks.length <= 1) return chunks;

  const result: SemanticChunk[] = [];

  for (const chunk of chunks) {
    if (result.length === 0) {
      result.push(chunk);
      continue;
    }

    const lastChunk = result[result.length - 1];

    // í˜„ì¬ ì²­í¬ê°€ ë„ˆë¬´ ì§§ê³ , ì´ì „ ì²­í¬ì™€ ê°™ì€ íƒ€ì…ì´ë©´ ë³‘í•©
    if (
      chunk.content.length < minSize &&
      chunk.type === lastChunk.type
    ) {
      lastChunk.content += '\n\n' + chunk.content;
      lastChunk.metadata.endOffset = chunk.metadata.endOffset;
      if (chunk.topic && !lastChunk.topic.includes(chunk.topic)) {
        lastChunk.topic += ', ' + chunk.topic;
      }
    } else {
      result.push(chunk);
    }
  }

  return result;
}

/**
 * ì²­í‚¹ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ê¸°ì¡´ í˜¸í™˜ì„±)
 */
export function calculateSemanticQualityScore(chunk: SemanticChunk): number {
  let score = 100;

  // ë„ˆë¬´ ì§§ìœ¼ë©´ ê°ì 
  if (chunk.content.length < 100) score -= 15;

  // ë„ˆë¬´ ê¸¸ë©´ ê°ì 
  if (chunk.content.length > 800) score -= 10;

  // Q&A íƒ€ì…ì´ë©´ ê°€ì‚°ì 
  if (chunk.type === 'qa') score += 10;

  // ì£¼ì œê°€ ëª…í™•í•˜ë©´ ê°€ì‚°ì 
  if (chunk.topic && chunk.topic.length > 2) score += 5;

  // ì˜ë¯¸ì—†ëŠ” ë‚´ìš©ì´ë©´ ê°ì 
  const meaningfulChars = chunk.content.replace(/[\d\s\W]/g, '');
  if (meaningfulChars.length < chunk.content.length * 0.3) {
    score -= 20;
  }

  return Math.max(0, Math.min(100, score));
}
```

### 3.2 Inngest íŒŒì´í”„ë¼ì¸ ìˆ˜ì •

`inngest/functions/process-document.ts` ìˆ˜ì •:

```typescript
// ê¸°ì¡´ importì— ì¶”ê°€
import {
  semanticChunk,
  isSemanticChunkingEnabled,
  calculateSemanticQualityScore,
  type SemanticChunk
} from '@/lib/rag/semantic-chunking';

// Step 3 ìˆ˜ì •
const chunkResults = await step.run('chunk-document', async () => {
  await updateDocumentProgress(documentId, 'chunking', 0);

  // AI Semantic Chunking ì‹œë„, ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜ í´ë°±
  if (isSemanticChunkingEnabled()) {
    const semanticChunks = await semanticChunk(
      parseResult.text,
      {
        minChunkSize: 100,
        maxChunkSize: 600,
        preChunkSize: 2000,
      },
      async (current, total) => {
        const progress = Math.round((current / total) * 100);
        await updateDocumentProgress(documentId, 'chunking', progress);
      },
      { tenantId }
    );

    return semanticChunks.map(chunk => ({
      content: chunk.content,
      index: chunk.index,
      qualityScore: calculateSemanticQualityScore(chunk),
      metadata: {
        ...chunk.metadata,
        type: chunk.type,
        topic: chunk.topic,
      },
    }));
  }

  // í´ë°±: ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜
  const chunksData = await smartChunk(parseResult.text, {
    maxChunkSize: 500,
    overlap: 50,
    preserveStructure: true,
  });

  await updateDocumentProgress(documentId, 'chunking', 100);
  return chunksData;
});
```

---

## 4. í™˜ê²½ ë³€ìˆ˜

```bash
# .env.local

# AI Semantic Chunking (ì´ë¯¸ Contextual Retrievalì—ì„œ ì‚¬ìš© ì¤‘)
ANTHROPIC_API_KEY=sk-ant-...

# ì„ íƒì : Semantic Chunking ê°•ì œ ë¹„í™œì„±í™”
# DISABLE_SEMANTIC_CHUNKING=true
```

---

## 5. í…ŒìŠ¤íŠ¸ ê³„íš

### 5.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```typescript
// __tests__/lib/rag/semantic-chunking.test.ts

describe('semanticChunk', () => {
  it('should split Q&A pairs correctly', async () => {
    const content = `
Q: ë°°ì†¡ì€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?
A: ì¼ë°˜ ë°°ì†¡ì€ 2-3ì¼, íŠ¹ê¸‰ ë°°ì†¡ì€ ë‹¹ì¼ ë„ì°©í•©ë‹ˆë‹¤.

Q: ë°˜í’ˆì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
A: 7ì¼ ì´ë‚´ ë°˜í’ˆ ê°€ëŠ¥í•˜ë©°, ê³ ê°ì„¼í„°ì— ì—°ë½í•˜ì‹œë©´ ë©ë‹ˆë‹¤.
    `;

    const chunks = await semanticChunk(content);

    expect(chunks).toHaveLength(2);
    expect(chunks[0].type).toBe('qa');
    expect(chunks[0].content).toContain('ë°°ì†¡');
    expect(chunks[1].type).toBe('qa');
    expect(chunks[1].content).toContain('ë°˜í’ˆ');
  });

  it('should not split short content', async () => {
    const content = 'ì§§ì€ ë‚´ìš©ì…ë‹ˆë‹¤.';
    const chunks = await semanticChunk(content);

    expect(chunks).toHaveLength(1);
  });

  it('should fallback to rule-based when API key missing', async () => {
    const originalKey = process.env.ANTHROPIC_API_KEY;
    delete process.env.ANTHROPIC_API_KEY;

    const chunks = await semanticChunk('í…ŒìŠ¤íŠ¸ ë‚´ìš©ì…ë‹ˆë‹¤.');

    expect(chunks.length).toBeGreaterThan(0);

    process.env.ANTHROPIC_API_KEY = originalKey;
  });
});
```

### 5.2 í†µí•© í…ŒìŠ¤íŠ¸

```typescript
// __tests__/integration/semantic-chunking.test.ts

describe('Semantic Chunking Pipeline', () => {
  it('should process document with semantic chunking', async () => {
    // 1. í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì—…ë¡œë“œ
    // 2. Inngest ì´ë²¤íŠ¸ íŠ¸ë¦¬ê±°
    // 3. ì²­í¬ ê²°ê³¼ ê²€ì¦
  });

  it('should track token usage correctly', async () => {
    // í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  ê²€ì¦
  });
});
```

### 5.3 A/B í…ŒìŠ¤íŠ¸ ê³„íš

| ê·¸ë£¹ | ì²­í‚¹ ë°©ì‹ | ì¸¡ì • ì§€í‘œ |
|------|-----------|-----------|
| A (Control) | ê·œì¹™ ê¸°ë°˜ | ê²€ìƒ‰ ì •í™•ë„, ì²­í¬ ìˆ˜ |
| B (Treatment) | AI Semantic | ê²€ìƒ‰ ì •í™•ë„, ì²­í¬ ìˆ˜ |

ì¸¡ì • ì§€í‘œ:
- ê²€ìƒ‰ ì •í™•ë„ (Precision@K)
- í‰ê·  ì²­í¬ ìˆ˜ (ë¬¸ì„œë‹¹)
- ì˜ë¯¸ì—†ëŠ” ì²­í¬ ë¹„ìœ¨
- ì²˜ë¦¬ ì‹œê°„
- ë¹„ìš©

---

## 6. ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

### 6.1 Phase 1: ì‹ ê·œ ë¬¸ì„œë§Œ ì ìš©
- ê¸°ì¡´ ë¬¸ì„œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
- ì‹ ê·œ ì—…ë¡œë“œ ë¬¸ì„œì—ë§Œ Semantic Chunking ì ìš©

### 6.2 Phase 2: ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜
- íŠ¹ì • ë°ì´í„°ì…‹/í…Œë„ŒíŠ¸ ì„ íƒì  ì¬ì²˜ë¦¬
- ê´€ë¦¬ì UIì—ì„œ "ì¬ì²˜ë¦¬" ë²„íŠ¼ìœ¼ë¡œ íŠ¸ë¦¬ê±°

### 6.3 Phase 3: ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜
- ì„±ëŠ¥ ê²€ì¦ í›„ ì „ì²´ ë¬¸ì„œ ì¬ì²˜ë¦¬
- ê¸°ì¡´ ì²­í¬ ì‚­ì œ â†’ ìƒˆ ì²­í¬ ìƒì„±

---

## 7. ë¹„ìš© ì˜ˆì¸¡

### 7.1 Semantic Chunking ë¹„ìš©

| í•­ëª© | ê³„ì‚° |
|------|------|
| í‰ê·  ë¬¸ì„œ í¬ê¸° | 10,000ì |
| Pre-chunk ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ | 5ê°œ (2000ìì”©) |
| Haiku ì…ë ¥ í† í° | ~3000 tokens/ì„¸ê·¸ë¨¼íŠ¸ |
| Haiku ì¶œë ¥ í† í° | ~500 tokens/ì„¸ê·¸ë¨¼íŠ¸ |
| **ë¬¸ì„œë‹¹ ë¹„ìš©** | ~$0.004 |

### 7.2 ê¸°ì¡´ ëŒ€ë¹„ ë¹„ìš© ë³€í™”

| ë‹¨ê³„ | í˜„ì¬ | ì œì•ˆ | ë³€í™” |
|------|------|------|------|
| ì²­í‚¹ | $0 | $0.004 | +$0.004 |
| Contextual | $0.008 | $0.006 (ì²­í¬ ê°ì†Œ) | -$0.002 |
| ì„ë² ë”© | $0.002 | $0.0015 (ì²­í¬ ê°ì†Œ) | -$0.0005 |
| **ì´ê³„** | $0.010 | $0.0115 | +15% |

---

## 8. ë¡¤ë°± ê³„íš

### 8.1 ì¦‰ì‹œ ë¡¤ë°±
```typescript
// í™˜ê²½ ë³€ìˆ˜ë¡œ ì¦‰ì‹œ ë¹„í™œì„±í™”
DISABLE_SEMANTIC_CHUNKING=true
```

### 8.2 ì½”ë“œ ë¡¤ë°±
- `isSemanticChunkingEnabled()` ë°˜í™˜ê°’ falseë¡œ ë³€ê²½
- Inngest íŒŒì´í”„ë¼ì¸ì´ ìë™ìœ¼ë¡œ ê·œì¹™ ê¸°ë°˜ í´ë°±

---

## 9. ì²´í¬ë¦¬ìŠ¤íŠ¸

### êµ¬í˜„ ì „
- [ ] ë¦¬ì„œì¹˜ ë¬¸ì„œ ê²€í†  ì™„ë£Œ
- [ ] ì„¤ê³„ ìŠ¹ì¸

### êµ¬í˜„
- [ ] `lib/rag/semantic-chunking.ts` ì‘ì„±
- [ ] Inngest íŒŒì´í”„ë¼ì¸ ìˆ˜ì •
- [ ] í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  ì¶”ê°€
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±

### í…ŒìŠ¤íŠ¸
- [ ] ë¡œì»¬ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ìŠ¤í…Œì´ì§• ë°°í¬ ë° í…ŒìŠ¤íŠ¸
- [ ] A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰

### ë°°í¬
- [ ] í”„ë¡œë•ì…˜ ë°°í¬
- [ ] ëª¨ë‹ˆí„°ë§ ì„¤ì •
- [ ] ë¡¤ë°± ê³„íš ì¤€ë¹„
